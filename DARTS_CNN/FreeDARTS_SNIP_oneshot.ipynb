{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment dir : FreeDARTS_SNIP_oneshot-exp-seed-9-20211008-031346\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import utils\n",
    "import logging\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torch.utils\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from model_search import Network\n",
    "from architect import Architect\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(\"cifar\")\n",
    "parser.add_argument('--data', type=str, default='/home/mzhang3/Data/DARTS/darts-master/data', help='location of the data corpus')\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='batch size')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.025, help='init learning rate')\n",
    "parser.add_argument('--learning_rate_min', type=float, default=0.001, help='min learning rate')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='momentum')\n",
    "parser.add_argument('--weight_decay', type=float, default=3e-4, help='weight decay')\n",
    "parser.add_argument('--report_freq', type=float, default=50, help='report frequency')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu device id')\n",
    "parser.add_argument('--epochs', type=int, default=50, help='num of training epochs')\n",
    "parser.add_argument('--init_channels', type=int, default=16, help='num of init channels')\n",
    "parser.add_argument('--layers', type=int, default=8, help='total number of layers')\n",
    "parser.add_argument('--model_path', type=str, default='saved_models', help='path to save the model')\n",
    "parser.add_argument('--cutout', action='store_true', default=False, help='use cutout')\n",
    "parser.add_argument('--cutout_length', type=int, default=16, help='cutout length')\n",
    "parser.add_argument('--drop_path_prob', type=float, default=0.3, help='drop path probability')\n",
    "parser.add_argument('--save', type=str, default='EXP', help='experiment name')\n",
    "parser.add_argument('--seed', type=int, default=9, help='random seed')\n",
    "parser.add_argument('--grad_clip', type=float, default=5, help='gradient clipping')\n",
    "parser.add_argument('--train_portion', type=float, default=0.5, help='portion of training data')\n",
    "parser.add_argument('--unrolled', action='store_true', default=False, help='use one-step unrolled validation loss')\n",
    "parser.add_argument('--arch_learning_rate', type=float, default=3e-4, help='learning rate for arch encoding')\n",
    "parser.add_argument('--arch_weight_decay', type=float, default=1e-3, help='weight decay for arch encoding')\n",
    "args = parser.parse_args([])\n",
    "\n",
    "args.save = 'FreeDARTS_SNIP_oneshot-exp-seed-{}-{}'.format(args.seed, time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "utils.create_exp_dir(args.save, scripts_to_save=glob.glob('*.py'))\n",
    "log_format = '%(asctime)s %(message)s'\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "fh = logging.FileHandler(os.path.join(args.save, 'log.txt'))\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "\n",
    "\n",
    "CIFAR_CLASSES = 10\n",
    "\n",
    "\n",
    "\n",
    "def pruning_func(train_queue,valid_queue, model, a_optimizer, criterion, optimizer, lr):\n",
    "    objs = utils.AvgrageMeter()\n",
    "    top1 = utils.AvgrageMeter()\n",
    "    top5 = utils.AvgrageMeter()\n",
    "    \n",
    "    ####-------------set \\theta abs\n",
    "    for name, param in model.state_dict().items():\n",
    "        param.abs_()\n",
    "    ####-------------set \\theta abs    \n",
    "    #model._arch_parameters[0].data=abs(model._arch_parameters[0].data)\n",
    "   # model._arch_parameters[1].data=abs(model._arch_parameters[1].data)\n",
    "    \n",
    "    for step, (input, target) in enumerate(train_queue):\n",
    "        model.train()\n",
    "        n = input.size(0)\n",
    "\n",
    "        input = Variable(input, requires_grad=False).cuda()\n",
    "        target = Variable(target, requires_grad=False).cuda(async=True)\n",
    "\n",
    "        # get a random minibatch from the search queue with replacement\n",
    "        input_search, target_search = next(iter(valid_queue))\n",
    "        input_search = Variable(input_search, requires_grad=False).cuda()\n",
    "        target_search = Variable(target_search, requires_grad=False).cuda(async=True)\n",
    "\n",
    "        a_optimizer.zero_grad()  \n",
    "        \n",
    "        logits = model(input_search)        \n",
    "        \n",
    "        arch_loss = criterion(logits, target_search)\n",
    "        arch_loss.backward()\n",
    "        #a_optimizer.step()         \n",
    "        print(arch_loss)\n",
    "\n",
    "\n",
    "        norm_arch_pruned= abs(model._arch_parameters[0].data*model._arch_parameters[0].grad.data)\n",
    "        reduce_arch_pruned= abs(model._arch_parameters[1].data*model._arch_parameters[1].grad.data)\n",
    "        \n",
    "        \n",
    "        break\n",
    "\n",
    "\n",
    "    return norm_arch_pruned, reduce_arch_pruned\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from operations import *\n",
    "from torch.autograd import Variable\n",
    "from genotypes import PRIMITIVES\n",
    "from genotypes import Genotype\n",
    "\n",
    "\n",
    "def synflow_genotype(norm_arch_synflow,redu_arch_synflow):\n",
    "    def _parse(weights):\n",
    "        gene = []\n",
    "        n = 2\n",
    "        start = 0\n",
    "        for i in range(4):\n",
    "            end = start + n\n",
    "            W = weights[start:end].copy()\n",
    "            edges = sorted(range(i + 2), key=lambda x: -max(W[x][k] for k in range(len(W[x])) if k != PRIMITIVES.index('none')))[:2]\n",
    "            for j in edges:\n",
    "                k_best = None\n",
    "                for k in range(len(W[j])):\n",
    "                    if k != PRIMITIVES.index('none'):\n",
    "                        if k_best is None or W[j][k] > W[j][k_best]:\n",
    "                            k_best = k\n",
    "                gene.append((PRIMITIVES[k_best], j))\n",
    "            start = end\n",
    "            n += 1\n",
    "        return gene\n",
    "\n",
    "    #gene_normal = _parse(F.softmax(self.alphas_normal, dim=-1).data.cpu().numpy())\n",
    "    #gene_reduce = _parse(F.softmax(self.alphas_reduce, dim=-1).data.cpu().numpy())\n",
    "    gene_normal = _parse(norm_arch_synflow.cpu().numpy())\n",
    "    gene_reduce = _parse(redu_arch_synflow.cpu().numpy())\n",
    "    \n",
    "    \n",
    "    concat = range(2, 6)\n",
    "    genotype = Genotype(\n",
    "      normal=gene_normal, normal_concat=concat,\n",
    "      reduce=gene_reduce, reduce_concat=concat\n",
    "    )\n",
    "    return genotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/08 03:13:47 AM gpu device = 0\n",
      "10/08 03:13:47 AM args = Namespace(arch_learning_rate=0.0003, arch_weight_decay=0.001, batch_size=64, cutout=False, cutout_length=16, data='/home/mzhang3/Data/DARTS/darts-master/data', drop_path_prob=0.3, epochs=50, gpu=0, grad_clip=5, init_channels=16, layers=8, learning_rate=0.025, learning_rate_min=0.001, model_path='saved_models', momentum=0.9, report_freq=50, save='FreeDARTS_SNIP_oneshot-exp-seed-9-20211008-031346', seed=9, train_portion=0.5, unrolled=False, weight_decay=0.0003)\n",
      "10/08 03:13:48 AM param size = 1.930618MB\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mzhang3/anaconda3/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/data/mzhang3/anaconda3/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:484: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n",
      "/pytorch/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of cuda is deprecated:\n",
      "\tcuda(torch.device device, bool async, *, torch.memory_format memory_format)\n",
      "Consider using one of the following signatures instead:\n",
      "\tcuda(torch.device device, bool non_blocking, *, torch.memory_format memory_format)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3068, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "10/08 03:13:50 AM Pruning cost 1.2 s.\n",
      "10/08 03:13:50 AM Searched architecture------------------\n",
      "10/08 03:13:50 AM Genotype(normal=[('sep_conv_3x3', 0), ('sep_conv_3x3', 1), ('sep_conv_3x3', 2), ('sep_conv_3x3', 0), ('sep_conv_3x3', 1), ('sep_conv_3x3', 3), ('sep_conv_5x5', 3), ('avg_pool_3x3', 4)], normal_concat=range(2, 6), reduce=[('dil_conv_3x3', 0), ('dil_conv_3x3', 1), ('sep_conv_5x5', 0), ('dil_conv_5x5', 2), ('dil_conv_5x5', 1), ('dil_conv_5x5', 2), ('sep_conv_5x5', 3), ('sep_conv_5x5', 0)], reduce_concat=range(2, 6))\n",
      "10/08 03:13:50 AM ---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    logging.info('no gpu device available')\n",
    "    sys.exit(1)\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.cuda.set_device(args.gpu)\n",
    "cudnn.benchmark = True\n",
    "torch.manual_seed(args.seed)\n",
    "cudnn.enabled=True\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "logging.info('gpu device = %d' % args.gpu)\n",
    "logging.info(\"args = %s\", args)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.cuda()\n",
    "model = Network(args.init_channels, CIFAR_CLASSES, args.layers, criterion)\n",
    "model = model.cuda()\n",
    "logging.info(\"param size = %fMB\", utils.count_parameters_in_MB(model))\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    args.learning_rate,\n",
    "    momentum=args.momentum,\n",
    "    weight_decay=args.weight_decay)\n",
    "\n",
    "\n",
    "a_optimizer = torch.optim.Adam(model.arch_parameters(),\n",
    "    lr=args.arch_learning_rate, betas=(0.5, 0.999), weight_decay=args.arch_weight_decay)\n",
    "\n",
    "\n",
    "\n",
    "train_transform, valid_transform = utils._data_transforms_cifar10(args)\n",
    "train_data = dset.CIFAR10(root=args.data, train=True, download=True, transform=train_transform)\n",
    "\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(args.train_portion * num_train))\n",
    "\n",
    "train_queue = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=args.batch_size,\n",
    "    sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[:split]),\n",
    "    pin_memory=True, num_workers=2)\n",
    "\n",
    "valid_queue = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=args.batch_size,\n",
    "    sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[split:num_train]),\n",
    "    pin_memory=True, num_workers=2)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, float(args.epochs), eta_min=args.learning_rate_min)\n",
    "\n",
    "architect = Architect(model, args)\n",
    "\n",
    "\n",
    "\n",
    "epoch=0\n",
    "\n",
    "scheduler.step()\n",
    "lr = scheduler.get_lr()[0]\n",
    "\n",
    "start_time= time.time()\n",
    "\n",
    "norm_arch_pruned, reduce_arch_pruned = pruning_func(train_queue,valid_queue, model, a_optimizer, criterion, optimizer, lr,)\n",
    "\n",
    "search_time = time.time() - start_time\n",
    "\n",
    "logging.info('Pruning cost {:.1f} s.'.format(search_time))\n",
    "\n",
    "genotype=synflow_genotype(norm_arch_pruned,reduce_arch_pruned)\n",
    "\n",
    "logging.info('Searched architecture------------------')\n",
    "logging.info(genotype)\n",
    "logging.info('---------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
